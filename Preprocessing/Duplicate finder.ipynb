{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/PKhosravi-CityTech/LightCnnRad/raw/main/Images/BioMindLogo.png\" alt=\"BioMind AI Lab Logo\" width=\"150\" height=\"150\" align=\"left\" style=\"margin-bottom: 40px;\"> **Repository Developed by Pegah Khosravi, Principal Investigator of the BioMind AI Lab**\n",
    "\n",
    "Welcome to this repository! This repository is a result of collaborative efforts from our dedicated team at the lab. We are committed to advancing the field of biomedical AI and pushing the boundaries of medical data analysis. Your interest and contributions to our work are greatly appreciated. For more information about our lab and ongoing projects, please visit the [BioMind AI Lab website](https://sites.google.com/view/biomind-ai-lab). Thank you for your interest and support!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code finds duplicate imgages and removes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "\n",
    "def compute_hash(image_path):\n",
    "    \"\"\"Compute a hash for an image.\"\"\"\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert('L')  # Ensure image is in grayscale mode\n",
    "        img_bytes = img.tobytes()\n",
    "        return hashlib.md5(img_bytes).hexdigest()\n",
    "\n",
    "def find_and_remove_duplicates(folder_path):\n",
    "   # Find and remove duplicate images in the specified folder, and report file counts.\n",
    "    hash_dict = {}\n",
    "    duplicates = []\n",
    "\n",
    "    all_files_before = [file for file in os.listdir(folder_path) if file.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    num_files_before = len(all_files_before)\n",
    "\n",
    "    for file in all_files_before:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        try:\n",
    "            img_hash = compute_hash(file_path)\n",
    "            if img_hash in hash_dict:\n",
    "                duplicates.append(file_path)\n",
    "            else:\n",
    "                hash_dict[img_hash] = file_path\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process {file}: {e}\")\n",
    "\n",
    "    # Remove duplicate images\n",
    "    for dup in duplicates:\n",
    "        try:\n",
    "            os.remove(dup)\n",
    "            print(f\"Removed duplicate image: {dup}\")\n",
    "        except Exception:\n",
    "            print(f\"Could not remove {dup}: {e}\")\n",
    "\n",
    "    # Get a list of all files in the folder after removal\n",
    "    all_files_after = [file for file in os.listdir(folder_path) if file.lower().endswith(('.jpg', '.jpeg', 'png'))]\n",
    "    num_files_after = len(all_files_after)\n",
    "\n",
    "    # Report the number of files before and after removal\n",
    "    print(f\"Number of files before removal: {num_files_before}\")\n",
    "    print(f\"Number of files after removal: {num_files_after}\")\n",
    "\n",
    "    if not duplicates:\n",
    "        print(\"No duplicate images found.\")\n",
    "\n",
    "# Example usage\n",
    "# Change this to your folder path\n",
    "folder_path = 'Datasets/Alzheimer MRI/Validation/Very_Mild_Demented'\n",
    "find_and_remove_duplicates(folder_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
