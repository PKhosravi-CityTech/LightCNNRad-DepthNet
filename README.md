# LightCNNRad-DepthNet

## About the Project

In this project, we introduce two CNN architectures, **LightCnnRad** and **DepthNet**, designed to optimize computational efficiency while maintaining high performance. These novel architectures are tailored specifically for radiological image analysis. Key highlights include:

- **Performance:** Comparable performance to the more complex pre-trained VGG-16 models.
- **Efficiency:** Significant reductions in computational complexity.
- **Versatility:** Successfully applied across various imaging modalities, including MRI, CT, X-ray, and Ultrasound.

Our findings demonstrate that simpler architectures can achieve competitive performance, highlighting the potential of **LightCnnRad** and **DepthNet** in clinical settings where computational resources and real-time processing are critical. This study addresses the limitations of traditional deep learning models, such as VGG-16, by proposing simplified architectures that maintain high performance while reducing computational demands.

## Usage

**LightCnnRad** and **DepthNet** can be employed in clinical settings to analyze a wide range of medical images, from MRI to Ultrasound. These algorithms are implemented using the PyTorch framework. To use these models, please ensure that you have:

1. Installed the PyTorch framework.
2. Installed other required Python libraries.

## Contribution

This project was supervised by **Dr. Pegah Khosravi** at the  [**BioMind AI lab**](https://sites.google.com/view/biomind-ai-lab). The following authors have contributed to this project:

- Saber Mohammadi
- Abhinita S. Mohanty
- Shady Saikali
- Doori Rose
- WintPyae LynnHtaik
- Raecine Greaves
- Tassadit Lounes
- Eshaan Haque
- Aashi Hirani
- Javad Zahiri
- Iman Dehzangi
- Vipul Patel
- Pegah Khosravi

## ðŸ“š How to Cite

When referencing this repository, please use the following citation format:

**S. Mohammadi et al., "Beyond Algorithms: The Impact of Simplified CNN Models and Multifactorial Influences on Radiological Image Analysis," 2024**
